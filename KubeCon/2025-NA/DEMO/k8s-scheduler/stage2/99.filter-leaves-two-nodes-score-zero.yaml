# ============================================================
# Comprehensive Test: Stage 2 Leaves Two Nodes, Stage 3 Score Meaningless
# ============================================================
# This demonstrates how Stage 2 (Filter) can leave multiple nodes,
# but Stage 3 (Score) preferences are still meaningless when none of
# the remaining nodes match the scoring criteria.
#
# Expected behavior:
# - Stage 2 Filter narrows down to 2 nodes: w1-k8s and w3-k8s
# - Stage 3 Score preferences (zone-c, HDD) don't match either node
# - Both nodes get 0 points from Stage 3 scoring
# - Final selection determined by other scoring plugins or node name order
# - Stage 3 preferences are effectively meaningless for the decision
# ============================================================

apiVersion: v1
kind: Pod
metadata:
  name: comprehensive-bypass-stage3-two-nodes
  labels:
    test: comprehensive
    scenario: bypass-stage3-two-nodes
spec:
  # ====== Stage 2 Filter (Hard Constraints) ======

  # Filter 1: Only SSD nodes (w1, w3, w5)
  nodeSelector:
    disktype: ssd

  # Filter 2: REQUIRED - Only zone-a or zone-b nodes
  affinity:
    nodeAffinity:
      # This constraint leaves w1-k8s (zone-a, SSD) and w3-k8s (zone-b, SSD)
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: In
            values:
            - zone-a  # w1-k8s, w2-k8s
            - zone-b  # w3-k8s, w4-k8s
          - key: disktype
            operator: In
            values:
            - ssd  # Combined: only w1-k8s and w3-k8s remain!

      # Result after Stage 2: w1-k8s (zone-a, SSD) and w3-k8s (zone-b, SSD)

      # ====== Stage 3 Score (Soft Constraints) - MEANINGLESS! ======

      # Prefer zone-c nodes (w5, w6)
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values:
            - zone-c  # ← w1 is zone-a, w3 is zone-b → Both get 0 points

      # Prefer HDD nodes (w2, w4, w6)
      - weight: 50
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - hdd  # ← Both w1 and w3 are SSD → Both get 0 points

  containers:
  - name: app
    image: quay.io/nginx/nginx-unprivileged:1.27.5-alpine-slim
    resources:
      requests:
        memory: "16Mi"
        cpu: "10m"

# ============================================================
# Expected Result:
# ============================================================
# Node: w1-k8s or w3-k8s (both are equally viable)
#
# Why Stage 3 is meaningless:
# 1. Stage 2 Filter left 2 candidates: w1-k8s (zone-a, SSD) and w3-k8s (zone-b, SSD)
# 2. Stage 3 Score preferences wanted zone-c + HDD
# 3. Neither w1 nor w3 match zone-c (both get 0 points from weight: 100)
# 4. Neither w1 nor w3 match HDD (both get 0 points from weight: 50)
# 5. Both nodes have identical Stage 3 scores: 0 points
# 6. Final decision made by:
#    - Other scoring plugins (NodeResourcesFit, ImageLocality, etc.)
#    - If still tied, node name order (w1-k8s comes before w3-k8s)
# 7. Stage 3 preferences had ZERO impact on the decision
#
# Comparison with 99.comprehensive-bypass-stage3.yaml:
# - 99: Stage 2 leaves 1 node → Stage 3 cannot change outcome
# - 98: Stage 2 leaves 2 nodes → Stage 3 scores both as 0 → Still meaningless
#
# Test commands:
# kubectl apply -f 98.comprehensive-bypass-stage3-two-nodes.yaml
# kubectl get pod comprehensive-bypass-stage3-two-nodes -o wide
# kubectl describe pod comprehensive-bypass-stage3-two-nodes | grep -A10 "Events:"
#
# Expected scheduling decision:
# Most likely w1-k8s due to:
# - Identical Stage 3 scores (0:0)
# - Other scoring factors or alphabetical node name ordering
